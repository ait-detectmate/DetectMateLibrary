{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"DetectMate Library Welcome to the documentation for the DetectMate Library . DetectMate is a flexible, component-based library for parsing and log anomaly detection. It can be used flexibly in research applications, and when used with the DetectMate Service , it can be easily deployed in microservice architectures. Getting started List of steps to follow for new users of the library: Installation : steps to install all the components need it. Basic usage : create a basic script with the different components. Create new component : guide to use the mate commands. Implement new component : guide to implement your own component. Components Documentation of the different components: Overall architecture : overall architecture of the library. Schemas : documentation of the different schemas in the library. Parsers : documentation of the different parsers. Detectors : documentation of the different detectors. Utils Tools that are used in the different components: Data Buffer : it takes the stream data and formated to the specifications given. Persistency : provides event-based state management for detectors. Helper Tools that their main objective is to help the developer: From_to : set of methods to save and load inputs and outputs from files.","title":"Home"},{"location":"#detectmate-library","text":"Welcome to the documentation for the DetectMate Library . DetectMate is a flexible, component-based library for parsing and log anomaly detection. It can be used flexibly in research applications, and when used with the DetectMate Service , it can be easily deployed in microservice architectures.","title":"DetectMate Library"},{"location":"#getting-started","text":"List of steps to follow for new users of the library: Installation : steps to install all the components need it. Basic usage : create a basic script with the different components. Create new component : guide to use the mate commands. Implement new component : guide to implement your own component.","title":"Getting started"},{"location":"#components","text":"Documentation of the different components: Overall architecture : overall architecture of the library. Schemas : documentation of the different schemas in the library. Parsers : documentation of the different parsers. Detectors : documentation of the different detectors.","title":"Components"},{"location":"#utils","text":"Tools that are used in the different components: Data Buffer : it takes the stream data and formated to the specifications given. Persistency : provides event-based state management for detectors.","title":"Utils"},{"location":"#helper","text":"Tools that their main objective is to help the developer: From_to : set of methods to save and load inputs and outputs from files.","title":"Helper"},{"location":"basic_usage/","text":"Getting started: Basic usage Basic usage of the DetectMate Library. Go back to Index , to previous step: Installation or to next step: Create new component .","title":"Basic usage"},{"location":"basic_usage/#getting-started-basic-usage","text":"Basic usage of the DetectMate Library. Go back to Index , to previous step: Installation or to next step: Create new component .","title":"Getting started: Basic usage"},{"location":"create_components/","text":"Getting started: Create new component DetectMateLibrary includes a small CLI helper to bootstrap standalone workspaces for custom parsers and detectors. This is useful if you want to develop and test components in isolation while still using the same library and schemas. Usage The CLI entry point is mate with a create command: mate create --type <parser|detector> --name <workspace_name> --dir <target_dir> Option Description --type Component type to generate: - parser : CoreParser-based template - detector : CoreDetector-based template --name Name of the component and package: - Creates package dir: <target_dir>/<name>/ - Creates main file: <name>.py - Derives class names: <Name> and <Name>Config --dir Directory where the workspace will be created What gets generated For example: mate create --type parser --name custom_parser --dir ./workspaces/custom_parser will create: workspaces/custom_parser/ # workspace root \u251c\u2500\u2500 custom_parser/ # Python package \u2502 \u251c\u2500\u2500 __init__.py \u2502 \u2514\u2500\u2500 custom_parser.py # CoreParser-based template \u251c\u2500\u2500 tests/ \u2502 \u2514\u2500\u2500 test_custom_parser.py # generated from template to test custom_parser \u251c\u2500\u2500 LICENSE.md # copied from main project \u251c\u2500\u2500 .gitignore # copied from main project \u251c\u2500\u2500 .pre-commit-config.yaml # copied from main project \u251c\u2500\u2500 pyproject.toml # minimal project + dev extras \u2514\u2500\u2500 README.md # setup instructions Go back to Index , to previous step: Basic usage or to next step: Implement new component .","title":"Create new component"},{"location":"create_components/#getting-started-create-new-component","text":"DetectMateLibrary includes a small CLI helper to bootstrap standalone workspaces for custom parsers and detectors. This is useful if you want to develop and test components in isolation while still using the same library and schemas.","title":"Getting started: Create new component"},{"location":"create_components/#usage","text":"The CLI entry point is mate with a create command: mate create --type <parser|detector> --name <workspace_name> --dir <target_dir> Option Description --type Component type to generate: - parser : CoreParser-based template - detector : CoreDetector-based template --name Name of the component and package: - Creates package dir: <target_dir>/<name>/ - Creates main file: <name>.py - Derives class names: <Name> and <Name>Config --dir Directory where the workspace will be created","title":"Usage"},{"location":"create_components/#what-gets-generated","text":"For example: mate create --type parser --name custom_parser --dir ./workspaces/custom_parser will create: workspaces/custom_parser/ # workspace root \u251c\u2500\u2500 custom_parser/ # Python package \u2502 \u251c\u2500\u2500 __init__.py \u2502 \u2514\u2500\u2500 custom_parser.py # CoreParser-based template \u251c\u2500\u2500 tests/ \u2502 \u2514\u2500\u2500 test_custom_parser.py # generated from template to test custom_parser \u251c\u2500\u2500 LICENSE.md # copied from main project \u251c\u2500\u2500 .gitignore # copied from main project \u251c\u2500\u2500 .pre-commit-config.yaml # copied from main project \u251c\u2500\u2500 pyproject.toml # minimal project + dev extras \u2514\u2500\u2500 README.md # setup instructions Go back to Index , to previous step: Basic usage or to next step: Implement new component .","title":"What gets generated"},{"location":"detectors/","text":"Components: Detectors Detectors process structured logs from Parsers and emit alerts when anomalies are detected. Schema Description Input ParserSchema Structured log Output DetectorSchema Alert / finding This document describes the minimal API, implementation guidance, a short example detector and a unit test pattern. CoreDetector \u2014 minimal API class CoreDetectorConfig(CoreConfig): comp_type: str = \"detectors\" method_type: str = \"core_detector\" parser: str = \"<PLACEHOLDER>\" auto_config: bool = False class CoreDetector(CoreComponent): def run( self, input_: List[ParserSchema] | ParserSchema, output_: DetectorSchema ) -> bool: \"\"\"Define in the Core detector\"\"\" def detect( self, input_: List[ParserSchema] | ParserSchema, output_: DetectorSchema, ) -> bool: \"\"\"Empty, must be define in the specific detector\"\"\" def train( self, input_: ParserSchema | list[ParserSchema] ) -> None: \"\"\"Empty, can be define in the detector. It trains the detector\"\"\" Implementing a detector \u2014 example Simple detector that raises an alert when a numeric variable exceeds a threshold. class SimpleThresholdConfig(CoreDetectorConfig): method_type: str = \"simple_threshold\" threshold: float = 0.0 class SimpleThresholdDetector(CoreDetector): def __init__( self, name: str = \"SimpleThreshold\", config: SimpleThresholdConfig | dict[str, Any] = SimpleThresholdConfig() ): if isinstance(config, dict): config = SimpleThresholdConfig.from_dict(config, name) super().__init__(name=name, buffer_mode=BufferMode.NO_BUF, config=config) def detect( self, input_: schemas.ParserSchema, output_: schemas.DetectorSchema ) -> bool: # calculate is a dummy method if calculate(input_) > self.config.threshold: output_[\"alertID\"] = f\"{self.name}-{int(time.time())}\" output_[\"logIDs\"].extend([ev.logID] if ev.logID else []) output_[\"score\"] = float(value) output_[\"description\"] = f\"Value {value} > threshold {self.config.threshold}\" return True return False To configure the number of logs receive as input, you need to configure the buffer in the initialization of the Detector. Detectors methods List of detectors: Random detector : Generates random alerts. New Value : Detect new values in the variables in the logs. Combo Detector : Detect new combination of variables in the logs. Auto-configuration (optional) Detectors can optionally support auto-configuration \u2014 a process where the detector automatically discovers which variables are worth monitoring, instead of requiring the user to specify them manually. Enabling auto-configuration Auto-configuration is controlled by the auto_config flag in the pipeline config (e.g. config/pipeline_config_default.yaml ): detectors: NewValueDetector: method_type: new_value_detector auto_config: True # enable auto-configuration params: {} # no \"events\" block needed \u2014 it will be generated automatically When auto_config is set to False , the detector expects an explicit events block that specifies exactly which variables to monitor: detectors: NewValueDetector: method_type: new_value_detector auto_config: False params: {} events: 1: instance1: params: {} variables: - pos: 0 name: var1 header_variables: - pos: level How it works When auto-configuration is enabled, the detector goes through two extra phases before training: Phase 1 \u2014 configure(input_) : The detector ingests events into an EventPersistency instance that uses a tracker backend to analyze variable behavior \u2014 for example, whether each variable is stable, random, or still has insufficient data. This instance is typically separate from the one used for training, because the configuration phase needs to observe all variables to decide which ones are worth monitoring, while training only tracks the variables that were selected as a result. Phase 2 \u2014 set_configuration() : After enough data has been ingested, the detector queries the tracker to select variables that meet its criteria (e.g. only stable variables). It then generates a full events configuration from those results and updates its own config. At this point auto_config is set to False in the generated config, since the configuration is now explicit. After these two phases, the detector proceeds with the normal train() and detect() lifecycle using the generated configuration. Implementation pattern A detector that supports auto-configuration typically creates a separate EventPersistency instance for this purpose (but doesn't have to): class MyDetector(CoreDetector): def __init__(self, ...): super().__init__(...) # main persistency for training / detection self.persistency = EventPersistency( event_data_class=EventStabilityTracker, ) # separate persistency for auto-configuration self.auto_conf_persistency = EventPersistency( event_data_class=EventStabilityTracker, ) The configure() method ingests all available variables (not just configured ones) so the tracker can assess each one: def configure(self, input_): self.auto_conf_persistency.ingest_event( event_id=input_[\"EventID\"], event_template=input_[\"template\"], variables=input_[\"variables\"], named_variables=input_[\"logFormatVariables\"], ) The set_configuration() method queries the tracker results and generates the final config: def set_configuration(self): variables = {} for event_id, tracker in self.auto_conf_persistency.get_events_data().items(): stable_vars = tracker.get_variables_by_classification(\"STABLE\") variables[event_id] = stable_vars config_dict = generate_detector_config( variable_selection=variables, detector_name=self.name, method_type=self.config.method_type, ) self.config = MyDetectorConfig.from_dict(config_dict, self.name) Full lifecycle with auto-configuration 1. configure(input_) # call for each event in the dataset 2. set_configuration() # finalize which variables to monitor 3. train(input_) # call for each event in the dataset 4. detect(input_, output_) # call for each event to detect anomalies When auto_config is False , steps 1 and 2 are skipped entirely. Go back Index","title":"Detectors"},{"location":"detectors/#components-detectors","text":"Detectors process structured logs from Parsers and emit alerts when anomalies are detected. Schema Description Input ParserSchema Structured log Output DetectorSchema Alert / finding This document describes the minimal API, implementation guidance, a short example detector and a unit test pattern.","title":"Components: Detectors"},{"location":"detectors/#coredetector-minimal-api","text":"class CoreDetectorConfig(CoreConfig): comp_type: str = \"detectors\" method_type: str = \"core_detector\" parser: str = \"<PLACEHOLDER>\" auto_config: bool = False class CoreDetector(CoreComponent): def run( self, input_: List[ParserSchema] | ParserSchema, output_: DetectorSchema ) -> bool: \"\"\"Define in the Core detector\"\"\" def detect( self, input_: List[ParserSchema] | ParserSchema, output_: DetectorSchema, ) -> bool: \"\"\"Empty, must be define in the specific detector\"\"\" def train( self, input_: ParserSchema | list[ParserSchema] ) -> None: \"\"\"Empty, can be define in the detector. It trains the detector\"\"\"","title":"CoreDetector \u2014 minimal API"},{"location":"detectors/#implementing-a-detector-example","text":"Simple detector that raises an alert when a numeric variable exceeds a threshold. class SimpleThresholdConfig(CoreDetectorConfig): method_type: str = \"simple_threshold\" threshold: float = 0.0 class SimpleThresholdDetector(CoreDetector): def __init__( self, name: str = \"SimpleThreshold\", config: SimpleThresholdConfig | dict[str, Any] = SimpleThresholdConfig() ): if isinstance(config, dict): config = SimpleThresholdConfig.from_dict(config, name) super().__init__(name=name, buffer_mode=BufferMode.NO_BUF, config=config) def detect( self, input_: schemas.ParserSchema, output_: schemas.DetectorSchema ) -> bool: # calculate is a dummy method if calculate(input_) > self.config.threshold: output_[\"alertID\"] = f\"{self.name}-{int(time.time())}\" output_[\"logIDs\"].extend([ev.logID] if ev.logID else []) output_[\"score\"] = float(value) output_[\"description\"] = f\"Value {value} > threshold {self.config.threshold}\" return True return False To configure the number of logs receive as input, you need to configure the buffer in the initialization of the Detector.","title":"Implementing a detector \u2014 example"},{"location":"detectors/#detectors-methods","text":"List of detectors: Random detector : Generates random alerts. New Value : Detect new values in the variables in the logs. Combo Detector : Detect new combination of variables in the logs.","title":"Detectors methods"},{"location":"detectors/#auto-configuration-optional","text":"Detectors can optionally support auto-configuration \u2014 a process where the detector automatically discovers which variables are worth monitoring, instead of requiring the user to specify them manually.","title":"Auto-configuration (optional)"},{"location":"detectors/#enabling-auto-configuration","text":"Auto-configuration is controlled by the auto_config flag in the pipeline config (e.g. config/pipeline_config_default.yaml ): detectors: NewValueDetector: method_type: new_value_detector auto_config: True # enable auto-configuration params: {} # no \"events\" block needed \u2014 it will be generated automatically When auto_config is set to False , the detector expects an explicit events block that specifies exactly which variables to monitor: detectors: NewValueDetector: method_type: new_value_detector auto_config: False params: {} events: 1: instance1: params: {} variables: - pos: 0 name: var1 header_variables: - pos: level","title":"Enabling auto-configuration"},{"location":"detectors/#how-it-works","text":"When auto-configuration is enabled, the detector goes through two extra phases before training: Phase 1 \u2014 configure(input_) : The detector ingests events into an EventPersistency instance that uses a tracker backend to analyze variable behavior \u2014 for example, whether each variable is stable, random, or still has insufficient data. This instance is typically separate from the one used for training, because the configuration phase needs to observe all variables to decide which ones are worth monitoring, while training only tracks the variables that were selected as a result. Phase 2 \u2014 set_configuration() : After enough data has been ingested, the detector queries the tracker to select variables that meet its criteria (e.g. only stable variables). It then generates a full events configuration from those results and updates its own config. At this point auto_config is set to False in the generated config, since the configuration is now explicit. After these two phases, the detector proceeds with the normal train() and detect() lifecycle using the generated configuration.","title":"How it works"},{"location":"detectors/#implementation-pattern","text":"A detector that supports auto-configuration typically creates a separate EventPersistency instance for this purpose (but doesn't have to): class MyDetector(CoreDetector): def __init__(self, ...): super().__init__(...) # main persistency for training / detection self.persistency = EventPersistency( event_data_class=EventStabilityTracker, ) # separate persistency for auto-configuration self.auto_conf_persistency = EventPersistency( event_data_class=EventStabilityTracker, ) The configure() method ingests all available variables (not just configured ones) so the tracker can assess each one: def configure(self, input_): self.auto_conf_persistency.ingest_event( event_id=input_[\"EventID\"], event_template=input_[\"template\"], variables=input_[\"variables\"], named_variables=input_[\"logFormatVariables\"], ) The set_configuration() method queries the tracker results and generates the final config: def set_configuration(self): variables = {} for event_id, tracker in self.auto_conf_persistency.get_events_data().items(): stable_vars = tracker.get_variables_by_classification(\"STABLE\") variables[event_id] = stable_vars config_dict = generate_detector_config( variable_selection=variables, detector_name=self.name, method_type=self.config.method_type, ) self.config = MyDetectorConfig.from_dict(config_dict, self.name)","title":"Implementation pattern"},{"location":"detectors/#full-lifecycle-with-auto-configuration","text":"1. configure(input_) # call for each event in the dataset 2. set_configuration() # finalize which variables to monitor 3. train(input_) # call for each event in the dataset 4. detect(input_, output_) # call for each event to detect anomalies When auto_config is False , steps 1 and 2 are skipped entirely. Go back Index","title":"Full lifecycle with auto-configuration"},{"location":"implement_components/","text":"Getting started: Implement new component Go back to Index or to previous step: Create new component .","title":"Implement new component"},{"location":"implement_components/#getting-started-implement-new-component","text":"Go back to Index or to previous step: Create new component .","title":"Getting started: Implement new component"},{"location":"installation/","text":"Getting started: Installation This document explains how to set up and run DetectMateLibrary for users and developers. User setup Purpose : install the library so you can import and use DetectMate in your projects. Recommended : use the provided uv helper (bundled Python environment manager used in this repo). If you prefer pip/venv, create a virtualenv first. Install the package in editable mode: uv sync Result : the package is installed into the active Python environment and changes to the source tree are reflected immediately. Developer setup Purpose : prepare a development environment with test and lint tooling. Step 1: Install Python development dependencies & pre-commit hooks Install dev dependencies (testing, linters, formatters): uv sync --dev Install pre-commit hooks (this repository uses prek to run pre-commit tooling): uv run --dev prek install Notes : Ensure uv is available in PATH. If not, use your system Python + virtualenv and then uv sync --dev . Run the pre-commit hooks locally with uv run --dev prek run -a before committing to catch style/typing issues early. Step 2: Install Protobuf toolchain (only if you change proto files) Purpose : compile .proto definitions into Python code. Install protoc on Debian/Ubuntu: sudo apt-get update sudo apt-get install -y protobuf-compiler protoc --version Compile the project proto: protoc \\ --proto_path=src/detectmatelibrary/schemas/ \\ --python_out=src/detectmatelibrary/schemas/ \\ src/detectmatelibrary/schemas/schemas.proto Result : generated Python modules appear under src/detectmatelibrary/schemas/ . If you edit proto files, re-run this command and commit generated code if required by your workflow. Step 3: Run unit tests Run the full test suite: uv run --dev pytest -s Run tests with coverage (terminal summary): uv run --dev pytest --cov=. --cov-report=term-missing Tips : Run a single test or directory to speed iteration: uv run --dev pytest tests/some_test.py::test_name -q Troubleshooting If uv is unavailable, use a Python virtualenv and the pip / pytest commands directly. If protoc is missing, install the system package or download a prebuilt binary for your OS. Always run commands from the project root so file paths (pyproject.toml, src/) resolve correctly. Go back to Index or proceed to Basic usage .","title":"Installation"},{"location":"installation/#getting-started-installation","text":"This document explains how to set up and run DetectMateLibrary for users and developers.","title":"Getting started: Installation"},{"location":"installation/#user-setup","text":"Purpose : install the library so you can import and use DetectMate in your projects. Recommended : use the provided uv helper (bundled Python environment manager used in this repo). If you prefer pip/venv, create a virtualenv first. Install the package in editable mode: uv sync Result : the package is installed into the active Python environment and changes to the source tree are reflected immediately.","title":"User setup"},{"location":"installation/#developer-setup","text":"Purpose : prepare a development environment with test and lint tooling.","title":"Developer setup"},{"location":"installation/#step-1-install-python-development-dependencies-pre-commit-hooks","text":"Install dev dependencies (testing, linters, formatters): uv sync --dev Install pre-commit hooks (this repository uses prek to run pre-commit tooling): uv run --dev prek install Notes : Ensure uv is available in PATH. If not, use your system Python + virtualenv and then uv sync --dev . Run the pre-commit hooks locally with uv run --dev prek run -a before committing to catch style/typing issues early.","title":"Step 1: Install Python development dependencies &amp; pre-commit hooks"},{"location":"installation/#step-2-install-protobuf-toolchain-only-if-you-change-proto-files","text":"Purpose : compile .proto definitions into Python code. Install protoc on Debian/Ubuntu: sudo apt-get update sudo apt-get install -y protobuf-compiler protoc --version Compile the project proto: protoc \\ --proto_path=src/detectmatelibrary/schemas/ \\ --python_out=src/detectmatelibrary/schemas/ \\ src/detectmatelibrary/schemas/schemas.proto Result : generated Python modules appear under src/detectmatelibrary/schemas/ . If you edit proto files, re-run this command and commit generated code if required by your workflow.","title":"Step 2: Install Protobuf toolchain (only if you change proto files)"},{"location":"installation/#step-3-run-unit-tests","text":"Run the full test suite: uv run --dev pytest -s Run tests with coverage (terminal summary): uv run --dev pytest --cov=. --cov-report=term-missing Tips : Run a single test or directory to speed iteration: uv run --dev pytest tests/some_test.py::test_name -q","title":"Step 3: Run unit tests"},{"location":"installation/#troubleshooting","text":"If uv is unavailable, use a Python virtualenv and the pip / pytest commands directly. If protoc is missing, install the system package or download a prebuilt binary for your OS. Always run commands from the project root so file paths (pyproject.toml, src/) resolve correctly. Go back to Index or proceed to Basic usage .","title":"Troubleshooting"},{"location":"overall_architecture/","text":"Overall architecture This document describes the high-level design of DetectMateLibrary, how components interact, the data contracts they use, and guidance for deploying and extending the system. The library is built around small, composable components that operate on streaming log data and exchange strongly-typed Schema objects. Key goals Clear separation of concerns (reading, parsing, detection, output). Stream-friendly processing with minimal buffering. Well-defined schema contracts so components can be composed or run as microservices. Easy extensibility: add new readers, parsers or detectors by subclassing core base classes. Components flow The pipeline is strictly directional: Parser : consumes raw logs and produces parsed log objects (structured fields, timestamps, variables). Detector : consumes parsed logs and generates alerts / findings when rules or models match anomalous behavior. Each arrow represents a stream of Schema objects . Components are designed to run in the same process for lightweight setups or as separate services for scalable deployments. Components architecture All components inherit from a CoreComponent class. This class provides all the essential functionality required for DetectMate to operate (see UML diagram below). Every Detector must inherit from CoreDetector , and every Parser must inherit from CoreParser to ensure compatibility with DetectMate. Each component\u2019s arguments must be stored in its corresponding configuration class. These config classes follow the same design pattern as their components and must inherit from CoreConfig . Components methods Each Core* base class exposes a small, stable API that implementations must implement or may override. class ConfigComponent(CoreConfig): \"\"\"Contains all the arguments of the component\"\"\" class Component(CoreComponent): def run( self, input_: List[BaseSchema] | BaseSchema, output_: BaseSchema ) -> bool: \"\"\"Run the component for a specific input\"\"\" def train( self, input_: List[BaseSchema] | BaseSchema, ) -> None: \"\"\"Train the component with a specific input\"\"\" def process(self, data: BaseSchema | bytes) -> BaseSchema | bytes | None: \"\"\"Process the data in a stream fashion (Defined in the CoreComponent)\"\"\" def get_config(self) -> Dict[str, Any]: \"\"\"\"Get the configuration of the component (Defined in the CoreComponent)\"\"\" def update_config(self, new_config: Dict[str, Any]) -> None: \"\"\"\"Update the configuration of the component (Defined in the CoreComponent)\"\"\" Go back Index","title":"Overall architecture"},{"location":"overall_architecture/#overall-architecture","text":"This document describes the high-level design of DetectMateLibrary, how components interact, the data contracts they use, and guidance for deploying and extending the system. The library is built around small, composable components that operate on streaming log data and exchange strongly-typed Schema objects. Key goals Clear separation of concerns (reading, parsing, detection, output). Stream-friendly processing with minimal buffering. Well-defined schema contracts so components can be composed or run as microservices. Easy extensibility: add new readers, parsers or detectors by subclassing core base classes.","title":"Overall architecture"},{"location":"overall_architecture/#components-flow","text":"The pipeline is strictly directional: Parser : consumes raw logs and produces parsed log objects (structured fields, timestamps, variables). Detector : consumes parsed logs and generates alerts / findings when rules or models match anomalous behavior. Each arrow represents a stream of Schema objects . Components are designed to run in the same process for lightweight setups or as separate services for scalable deployments.","title":"Components flow"},{"location":"overall_architecture/#components-architecture","text":"All components inherit from a CoreComponent class. This class provides all the essential functionality required for DetectMate to operate (see UML diagram below). Every Detector must inherit from CoreDetector , and every Parser must inherit from CoreParser to ensure compatibility with DetectMate. Each component\u2019s arguments must be stored in its corresponding configuration class. These config classes follow the same design pattern as their components and must inherit from CoreConfig .","title":"Components architecture"},{"location":"overall_architecture/#components-methods","text":"Each Core* base class exposes a small, stable API that implementations must implement or may override. class ConfigComponent(CoreConfig): \"\"\"Contains all the arguments of the component\"\"\" class Component(CoreComponent): def run( self, input_: List[BaseSchema] | BaseSchema, output_: BaseSchema ) -> bool: \"\"\"Run the component for a specific input\"\"\" def train( self, input_: List[BaseSchema] | BaseSchema, ) -> None: \"\"\"Train the component with a specific input\"\"\" def process(self, data: BaseSchema | bytes) -> BaseSchema | bytes | None: \"\"\"Process the data in a stream fashion (Defined in the CoreComponent)\"\"\" def get_config(self) -> Dict[str, Any]: \"\"\"\"Get the configuration of the component (Defined in the CoreComponent)\"\"\" def update_config(self, new_config: Dict[str, Any]) -> None: \"\"\"\"Update the configuration of the component (Defined in the CoreComponent)\"\"\" Go back Index","title":"Components methods"},{"location":"parsers/","text":"Components: Parsers Parsers convert unstructured raw logs into structured ParserSchema objects that downstream detectors consume. Schema Description Input LogSchema Unstructured log Output ParserSchema Structured log This document explains expected APIs, how to implement a parser, testing tips and common pitfalls. Overview Parsers must inherit from CoreParser and provide a parse() implementation. CoreParser.run() handles lifecycle and calls parse() for each input; implement pure parsing logic inside parse() where possible. Use a typed Config class (subclass of CoreParserConfig ) to hold runtime parameters. CoreParser \u2014 minimal API Recommended signatures and behavior: class CoreParser: def run(self, input_: schemas.LogSchema, output_: schemas.ParserSchema) -> bool: \"\"\"Top-level runner. Calls parse() and performs pre/post processing. Return True when a parsed output was produced, False otherwise. \"\"\" def parse(self, input_: schemas.LogSchema, output_: schemas.ParserSchema) -> bool: \"\"\"Implement parsing here. - Fill required output_ fields (see ParserSchema table below). - Return True if parsing succeeded and output_ contains a result. \"\"\" def train(self, input_: Iterable[schemas.LogSchema]) -> None: \"\"\"Optional: train internal models. Can be a no-op for stateless parsers.\"\"\" ParserSchema \u2014 what to populate Minimum fields commonly expected by downstream components: EventID (int) \u2014 identifier for the matched template/event template (string) \u2014 event template text variables (repeated string) \u2014 extracted parameters (extend the list) parsedLogID / logID \u2014 identifiers linking raw and parsed records parsedTimestamp / receivedTimestamp \u2014 timestamps Creating a new parser \u2014 step by step Create a Config class inheriting CoreParserConfig . Create parser class inheriting CoreParser . Implement parse() to populate output_ . Add unit tests for parse() behavior. Example: # filepath: src/detectmatelibrary/parsers/my_parser.py from detectmatelibrary.common.parser import CoreParser, CoreParserConfig from detectmatelibrary import schemas from typing import Any class MyParserConfig(CoreParserConfig): method_type: str = \"my_parser\" # add parser-specific settings here pattern: str | None = None class MyParser(CoreParser): def __init__(self, name: str = \"MyParser\", config: MyParserConfig | dict[str, Any] = MyParserConfig()): if isinstance(config, dict): config = MyParserConfig.from_dict(config, name) super().__init__(name=name, config=config) def parse(self, input_: schemas.LogSchema, output_: schemas.ParserSchema) -> bool: text = input_.log or \"\" # simple example: split on whitespace and treat as variables tokens = text.split() output_[\"EventID\"] = 1 output_[\"template\"] = \" \".join([\"<*>\"] * len(tokens)) output_[\"variables\"].extend(tokens) output_[\"parsedTimestamp\"] = int(time.time()) return True Testing parsers Unit test parse() directly using ParserSchema objects: def test_my_parser_parse(): from detectmatelibrary.parsers.my_parser import MyParser from detectmatelibrary import schemas parser = MyParser() raw = schemas.LogSchema({\"log\": \"a b c\", \"logID\": \"1\"}) out = schemas.ParserSchema() ok = parser.parse(raw, out) assert ok is True assert out[\"EventID\"] == 1 assert out[\"variables\"] == [\"a\", \"b\", \"c\"] Go back to Index","title":"Parsers"},{"location":"parsers/#components-parsers","text":"Parsers convert unstructured raw logs into structured ParserSchema objects that downstream detectors consume. Schema Description Input LogSchema Unstructured log Output ParserSchema Structured log This document explains expected APIs, how to implement a parser, testing tips and common pitfalls.","title":"Components: Parsers"},{"location":"parsers/#overview","text":"Parsers must inherit from CoreParser and provide a parse() implementation. CoreParser.run() handles lifecycle and calls parse() for each input; implement pure parsing logic inside parse() where possible. Use a typed Config class (subclass of CoreParserConfig ) to hold runtime parameters.","title":"Overview"},{"location":"parsers/#coreparser-minimal-api","text":"Recommended signatures and behavior: class CoreParser: def run(self, input_: schemas.LogSchema, output_: schemas.ParserSchema) -> bool: \"\"\"Top-level runner. Calls parse() and performs pre/post processing. Return True when a parsed output was produced, False otherwise. \"\"\" def parse(self, input_: schemas.LogSchema, output_: schemas.ParserSchema) -> bool: \"\"\"Implement parsing here. - Fill required output_ fields (see ParserSchema table below). - Return True if parsing succeeded and output_ contains a result. \"\"\" def train(self, input_: Iterable[schemas.LogSchema]) -> None: \"\"\"Optional: train internal models. Can be a no-op for stateless parsers.\"\"\"","title":"CoreParser \u2014 minimal API"},{"location":"parsers/#parserschema-what-to-populate","text":"Minimum fields commonly expected by downstream components: EventID (int) \u2014 identifier for the matched template/event template (string) \u2014 event template text variables (repeated string) \u2014 extracted parameters (extend the list) parsedLogID / logID \u2014 identifiers linking raw and parsed records parsedTimestamp / receivedTimestamp \u2014 timestamps","title":"ParserSchema \u2014 what to populate"},{"location":"parsers/#creating-a-new-parser-step-by-step","text":"Create a Config class inheriting CoreParserConfig . Create parser class inheriting CoreParser . Implement parse() to populate output_ . Add unit tests for parse() behavior. Example: # filepath: src/detectmatelibrary/parsers/my_parser.py from detectmatelibrary.common.parser import CoreParser, CoreParserConfig from detectmatelibrary import schemas from typing import Any class MyParserConfig(CoreParserConfig): method_type: str = \"my_parser\" # add parser-specific settings here pattern: str | None = None class MyParser(CoreParser): def __init__(self, name: str = \"MyParser\", config: MyParserConfig | dict[str, Any] = MyParserConfig()): if isinstance(config, dict): config = MyParserConfig.from_dict(config, name) super().__init__(name=name, config=config) def parse(self, input_: schemas.LogSchema, output_: schemas.ParserSchema) -> bool: text = input_.log or \"\" # simple example: split on whitespace and treat as variables tokens = text.split() output_[\"EventID\"] = 1 output_[\"template\"] = \" \".join([\"<*>\"] * len(tokens)) output_[\"variables\"].extend(tokens) output_[\"parsedTimestamp\"] = int(time.time()) return True","title":"Creating a new parser \u2014 step by step"},{"location":"parsers/#testing-parsers","text":"Unit test parse() directly using ParserSchema objects: def test_my_parser_parse(): from detectmatelibrary.parsers.my_parser import MyParser from detectmatelibrary import schemas parser = MyParser() raw = schemas.LogSchema({\"log\": \"a b c\", \"logID\": \"1\"}) out = schemas.ParserSchema() ok = parser.parse(raw, out) assert ok is True assert out[\"EventID\"] == 1 assert out[\"variables\"] == [\"a\", \"b\", \"c\"] Go back to Index","title":"Testing parsers"},{"location":"schemas/","text":"Schemas Schemas are the typed message objects used to transmit data between components (Parser \u2192 Detector). Each schema class wraps a Protobuf message and provides a small, convenient Python API for creation, inspection, (de)serialization and validation. This document summarizes the available schema classes, the BaseSchema API and common usage patterns. Design goals Strongly-typed contracts between components. Lightweight wrapper over generated Protobuf types. Simple API for tests and runtime wiring. Safe (de)serialization for transport and persistence. Architecture Schemas are used to transfer data between components. Each schema implements the methods defined in BaseSchema . This class acts as a wrapper around the underlying Protobuf classes ( op.SchemaT ). All concrete schema classes inherit from BaseSchema . Key utility methods: class BaseSchema: def __contains__(self, idx: str) -> bool: \"\"\"Return if a variable is in the schema\"\"\" def as_dict(self) -> dict[str, Any]: \"\"\"Return the schema variables as a dictionary.\"\"\" def get_schema(self) -> op.SchemaT: \"\"\"Retrieve the current schema instance.\"\"\" def set_schema(self, schema: op.SchemaT) -> None: \"\"\"Set the schema instance and update attributes.\"\"\" def init_schema(self, kwargs: dict[str, Any] | None) -> None: \"\"\"Initialize the schema instance and set attributes.\"\"\" self.var_names = set(var_names) def is_field_list(self, field_name: str) -> bool: \"\"\"Check if a field is a list.\"\"\" def copy(self) -> \"BaseSchema\": \"\"\"Create a deep copy of the schema instance.\"\"\" def serialize(self) -> bytes: \"\"\"Serialize the schema instance to bytes.\"\"\" def deserialize(self, message: bytes) -> None | op.IncorrectSchema: \"\"\"Deserialize bytes to populate the schema instance.\"\"\" def check_is_same(self, other: Self) -> None | op.IncorrectSchema: \"\"\"Check if another schema instance is of the same schema type.\"\"\" def __eq__(self, other: object) -> bool: \"\"\"Check equality between two schema instances.\"\"\" Schema Clases Below are the primary schema classes and their main fields. All fields are optional at the Protobuf level; components should document which fields they require. LogSchema Represents a raw log message. Fields: Field Type Notes logID string Unique identifier for the raw log. log string Raw log text. logSource string Source of the log (file, topic, etc.). hostname string Hostname where log originated. ParserSchema Output of a Parser. Contains parsed fields and template information. Fields: Field Type Notes parserType string Parser type. parserID string Parser instance identifier. EventID int32 Template/event identifier. template string Event template text. variables repeated string Parameters extracted from the template. parsedLogID string ID assigned after parsing (optional). logID string Original raw log ID (link to LogSchema). log string Raw log text. logFormatVariables map Key/value pairs from format extraction. receivedTimestamp int32 Timestamp when log was received. parsedTimestamp int32 Timestamp when parsing completed. DetectorSchema Output from Detectors (alerts / findings). Fields: Field Type Notes detectorID string Detector instance identifier. detectorType string Type/name of detector. alertID string Unique alert identifier. detectionTimestamp int32 When the alert was produced. logIDs repeated string IDs of logs related to the alert. score float Confidence/score (if applicable). extractedTimestamps repeated int32 Timestamps extracted from logs. description string Human-readable description of the alert. receivedTimestamp int32 When inputs were received by detector. alertsObtain map Additional alert metadata. Tutorial Small tutorials of the different schemas. Initialize a schema from detectmatelibrary import schemas kwargs = load_somewhere() # load the dict kwargs[\"log\"] = \"Test log\" log_schema = LogSchema(kwargs) print(log_schema.log == \"Test log\") # True Assign values from detectmatelibrary import schemas log_schema = LogSchema() log_schema.log = \"Test log\" print(log_schema[\"log\"] == log_schema.log) # True log_schema2 = LogSchema() print(log_schema == log_schema2) # False log_schema2.log = \"Test log\" print(log_schema == log_schema2) # True Serialization from detectmatelibrary import schemas log_schema = LogSchema() log_schema.log = \"Test log\" serialized = log_schema.serialize() print(isinstance(serialized, bytes)) # True new_log_schema = LogSchema() new_log_schema.deserialize(serialized) print(new_log_schema.schema_id == log_schema.schema_id) # True Go back Index","title":"Schemas"},{"location":"schemas/#schemas","text":"Schemas are the typed message objects used to transmit data between components (Parser \u2192 Detector). Each schema class wraps a Protobuf message and provides a small, convenient Python API for creation, inspection, (de)serialization and validation. This document summarizes the available schema classes, the BaseSchema API and common usage patterns.","title":"Schemas"},{"location":"schemas/#design-goals","text":"Strongly-typed contracts between components. Lightweight wrapper over generated Protobuf types. Simple API for tests and runtime wiring. Safe (de)serialization for transport and persistence.","title":"Design goals"},{"location":"schemas/#architecture","text":"Schemas are used to transfer data between components. Each schema implements the methods defined in BaseSchema . This class acts as a wrapper around the underlying Protobuf classes ( op.SchemaT ). All concrete schema classes inherit from BaseSchema . Key utility methods: class BaseSchema: def __contains__(self, idx: str) -> bool: \"\"\"Return if a variable is in the schema\"\"\" def as_dict(self) -> dict[str, Any]: \"\"\"Return the schema variables as a dictionary.\"\"\" def get_schema(self) -> op.SchemaT: \"\"\"Retrieve the current schema instance.\"\"\" def set_schema(self, schema: op.SchemaT) -> None: \"\"\"Set the schema instance and update attributes.\"\"\" def init_schema(self, kwargs: dict[str, Any] | None) -> None: \"\"\"Initialize the schema instance and set attributes.\"\"\" self.var_names = set(var_names) def is_field_list(self, field_name: str) -> bool: \"\"\"Check if a field is a list.\"\"\" def copy(self) -> \"BaseSchema\": \"\"\"Create a deep copy of the schema instance.\"\"\" def serialize(self) -> bytes: \"\"\"Serialize the schema instance to bytes.\"\"\" def deserialize(self, message: bytes) -> None | op.IncorrectSchema: \"\"\"Deserialize bytes to populate the schema instance.\"\"\" def check_is_same(self, other: Self) -> None | op.IncorrectSchema: \"\"\"Check if another schema instance is of the same schema type.\"\"\" def __eq__(self, other: object) -> bool: \"\"\"Check equality between two schema instances.\"\"\"","title":"Architecture"},{"location":"schemas/#schema-clases","text":"Below are the primary schema classes and their main fields. All fields are optional at the Protobuf level; components should document which fields they require.","title":"Schema Clases"},{"location":"schemas/#logschema","text":"Represents a raw log message. Fields: Field Type Notes logID string Unique identifier for the raw log. log string Raw log text. logSource string Source of the log (file, topic, etc.). hostname string Hostname where log originated.","title":"LogSchema"},{"location":"schemas/#parserschema","text":"Output of a Parser. Contains parsed fields and template information. Fields: Field Type Notes parserType string Parser type. parserID string Parser instance identifier. EventID int32 Template/event identifier. template string Event template text. variables repeated string Parameters extracted from the template. parsedLogID string ID assigned after parsing (optional). logID string Original raw log ID (link to LogSchema). log string Raw log text. logFormatVariables map Key/value pairs from format extraction. receivedTimestamp int32 Timestamp when log was received. parsedTimestamp int32 Timestamp when parsing completed.","title":"ParserSchema"},{"location":"schemas/#detectorschema","text":"Output from Detectors (alerts / findings). Fields: Field Type Notes detectorID string Detector instance identifier. detectorType string Type/name of detector. alertID string Unique alert identifier. detectionTimestamp int32 When the alert was produced. logIDs repeated string IDs of logs related to the alert. score float Confidence/score (if applicable). extractedTimestamps repeated int32 Timestamps extracted from logs. description string Human-readable description of the alert. receivedTimestamp int32 When inputs were received by detector. alertsObtain map Additional alert metadata.","title":"DetectorSchema"},{"location":"schemas/#tutorial","text":"Small tutorials of the different schemas.","title":"Tutorial"},{"location":"schemas/#initialize-a-schema","text":"from detectmatelibrary import schemas kwargs = load_somewhere() # load the dict kwargs[\"log\"] = \"Test log\" log_schema = LogSchema(kwargs) print(log_schema.log == \"Test log\") # True","title":"Initialize a schema"},{"location":"schemas/#assign-values","text":"from detectmatelibrary import schemas log_schema = LogSchema() log_schema.log = \"Test log\" print(log_schema[\"log\"] == log_schema.log) # True log_schema2 = LogSchema() print(log_schema == log_schema2) # False log_schema2.log = \"Test log\" print(log_schema == log_schema2) # True","title":"Assign values"},{"location":"schemas/#serialization","text":"from detectmatelibrary import schemas log_schema = LogSchema() log_schema.log = \"Test log\" serialized = log_schema.serialize() print(isinstance(serialized, bytes)) # True new_log_schema = LogSchema() new_log_schema.deserialize(serialized) print(new_log_schema.schema_id == log_schema.schema_id) # True Go back Index","title":"Serialization"},{"location":"auxiliar/input_buffer/","text":"Data Buffer The data buffer is an auxiliar methods that can be use in all the components. It takes the stream data and formated to the specifications given. It has different configuration states to configure its behaviour. Type State Description No buffer BufferMode.NO_BUF Returns one value at the time. Batch BufferMode.BATCH Returns values by batches. Window BufferMode.WINDOW Returns values by time windows. Examples Code examples to show the behaviour of the DataBuffer class. No Buffer mode from detectmatelibrary.utils.data_buffer import DataBuffer, ArgsBuffer, BufferMode results = [] buf = DataBuffer(ArgsBuffer(mode=BufferMode.NO_BUF, process_function=results.append)) buf.add(1) buf.add(2) print(results) # [1, 2] Batch mode from detectmatelibrary.utils.data_buffer import DataBuffer, ArgsBuffer, BufferMode results = [] buf = DataBuffer(ArgsBuffer(mode=BufferMode.BATCH, process_function=results.append, size=3)) buf.add(1) print(results) # [] buf.add(1) print(results) # [] buf.add(1) print(results) # [[1, 1, 1]] Window mode from detectmatelibrary.utils.data_buffer import DataBuffer, ArgsBuffer, BufferMode buf = DataBuffer(ArgsBuffer(mode=BufferMode.WINDOW, process_function=sum, size=2)) print(buf.add(1) is None) # True print(buf.add(2)) # 3 print(buf.add(5)) # 7 Go back Index","title":"Data buffer"},{"location":"auxiliar/input_buffer/#data-buffer","text":"The data buffer is an auxiliar methods that can be use in all the components. It takes the stream data and formated to the specifications given. It has different configuration states to configure its behaviour. Type State Description No buffer BufferMode.NO_BUF Returns one value at the time. Batch BufferMode.BATCH Returns values by batches. Window BufferMode.WINDOW Returns values by time windows.","title":"Data Buffer"},{"location":"auxiliar/input_buffer/#examples","text":"Code examples to show the behaviour of the DataBuffer class.","title":"Examples"},{"location":"auxiliar/input_buffer/#no-buffer-mode","text":"from detectmatelibrary.utils.data_buffer import DataBuffer, ArgsBuffer, BufferMode results = [] buf = DataBuffer(ArgsBuffer(mode=BufferMode.NO_BUF, process_function=results.append)) buf.add(1) buf.add(2) print(results) # [1, 2]","title":"No Buffer mode"},{"location":"auxiliar/input_buffer/#batch-mode","text":"from detectmatelibrary.utils.data_buffer import DataBuffer, ArgsBuffer, BufferMode results = [] buf = DataBuffer(ArgsBuffer(mode=BufferMode.BATCH, process_function=results.append, size=3)) buf.add(1) print(results) # [] buf.add(1) print(results) # [] buf.add(1) print(results) # [[1, 1, 1]]","title":"Batch mode"},{"location":"auxiliar/input_buffer/#window-mode","text":"from detectmatelibrary.utils.data_buffer import DataBuffer, ArgsBuffer, BufferMode buf = DataBuffer(ArgsBuffer(mode=BufferMode.WINDOW, process_function=sum, size=2)) print(buf.add(1) is None) # True print(buf.add(2)) # 3 print(buf.add(5)) # 7 Go back Index","title":"Window mode"},{"location":"auxiliar/persistency/","text":"Persistency The persistency module provides event-based state management for detectors. It allows detectors to accumulate, store, and query data across their lifecycle \u2014 during training, detection, and auto-configuration. EventPersistency EventPersistency is the main entry point. It manages one storage backend instance per event ID, so each event type maintains its own isolated state. Creating an instance from detectmatelibrary.common.persistency import EventPersistency persistency = EventPersistency( event_data_class=MyBackend, # storage backend class (see below) variable_blacklist=[\"Content\"], # variable names to exclude (optional) event_data_kwargs={\"max_rows\": 1000} # extra kwargs forwarded to the backend (optional) ) Parameter Description event_data_class An EventDataStructure subclass that defines how data is stored and queried. variable_blacklist Variable names to exclude from storage. Defaults to [\"Content\"] . event_data_kwargs A dictionary of keyword arguments forwarded to the backend constructor. Storing data persistency.ingest_event( event_id=event_id, event_template=template, variables=positional_vars, # optional positional variables named_variables=named_vars # optional named variables ) Each call appends data to the backend associated with the given event_id . If no backend exists for that ID yet, one is created automatically. Retrieving data # Single event data = persistency.get_event_data(event_id) # All events all_data = persistency.get_events_data() # dict[event_id -> backend] # Templates template = persistency.get_event_template(event_id) all_templates = persistency.get_event_templates() # Bracket access backend = persistency[event_id] Storage backends The backend determines how ingested data is stored and what queries are available. Choose the backend that fits your detector's needs. DataFrame backends Store raw event data in tabular form. Useful when a detector needs to query or iterate over historical values. EventDataFrame \u2014 Pandas-backed storage. Simple and familiar. ChunkedEventDataFrame \u2014 Polars-backed storage with configurable row retention and automatic compaction. Suited for high-volume or streaming workloads. from detectmatelibrary.common.persistency.event_data_structures.dataframes import ( EventDataFrame, ChunkedEventDataFrame, ) Tracker backends Track variable behavior over time rather than storing raw data. Useful when a detector needs to understand how variables evolve (e.g., whether they converge to constant values). Is optimized for space efficiency since only extracted features from the logs are stored. EventStabilityTracker \u2014 Classifies each variable as STATIC , STABLE , UNSTABLE , RANDOM , or INSUFFICIENT_DATA based on how its values change over time. from detectmatelibrary.common.persistency.event_data_structures.trackers import ( EventStabilityTracker, ) Usage in detectors Persistency is optional . A detector can function without it. When a detector does need to maintain state across events \u2014 for example, to learn normal values during training and flag deviations during detection \u2014 it can integrate persistency by following this pattern: 1. Initialize in __init__ Create one or more EventPersistency instances with the appropriate backend. class MyDetector(CoreDetector): def __init__(self, name=\"MyDetector\", config=MyDetectorConfig()): super().__init__(name=name, ...) self.persistency = EventPersistency( event_data_class=EventStabilityTracker, ) 2. Accumulate state in train() During training, ingest each event so the backend builds up its internal state. def train(self, input_): variables = self.get_configured_variables(input_, self.config.events) self.persistency.ingest_event( event_id=input_[\"EventID\"], event_template=input_[\"template\"], named_variables=variables, ) 3. Query state in detect() During detection, query the accumulated state to decide whether the incoming event is anomalous. def detect(self, input_, output_): for event_id, backend in self.persistency.get_events_data().items(): stored_data = backend.get_data() # compare input_ against stored_data to produce alerts","title":"Persistency"},{"location":"auxiliar/persistency/#persistency","text":"The persistency module provides event-based state management for detectors. It allows detectors to accumulate, store, and query data across their lifecycle \u2014 during training, detection, and auto-configuration.","title":"Persistency"},{"location":"auxiliar/persistency/#eventpersistency","text":"EventPersistency is the main entry point. It manages one storage backend instance per event ID, so each event type maintains its own isolated state.","title":"EventPersistency"},{"location":"auxiliar/persistency/#creating-an-instance","text":"from detectmatelibrary.common.persistency import EventPersistency persistency = EventPersistency( event_data_class=MyBackend, # storage backend class (see below) variable_blacklist=[\"Content\"], # variable names to exclude (optional) event_data_kwargs={\"max_rows\": 1000} # extra kwargs forwarded to the backend (optional) ) Parameter Description event_data_class An EventDataStructure subclass that defines how data is stored and queried. variable_blacklist Variable names to exclude from storage. Defaults to [\"Content\"] . event_data_kwargs A dictionary of keyword arguments forwarded to the backend constructor.","title":"Creating an instance"},{"location":"auxiliar/persistency/#storing-data","text":"persistency.ingest_event( event_id=event_id, event_template=template, variables=positional_vars, # optional positional variables named_variables=named_vars # optional named variables ) Each call appends data to the backend associated with the given event_id . If no backend exists for that ID yet, one is created automatically.","title":"Storing data"},{"location":"auxiliar/persistency/#retrieving-data","text":"# Single event data = persistency.get_event_data(event_id) # All events all_data = persistency.get_events_data() # dict[event_id -> backend] # Templates template = persistency.get_event_template(event_id) all_templates = persistency.get_event_templates() # Bracket access backend = persistency[event_id]","title":"Retrieving data"},{"location":"auxiliar/persistency/#storage-backends","text":"The backend determines how ingested data is stored and what queries are available. Choose the backend that fits your detector's needs.","title":"Storage backends"},{"location":"auxiliar/persistency/#dataframe-backends","text":"Store raw event data in tabular form. Useful when a detector needs to query or iterate over historical values. EventDataFrame \u2014 Pandas-backed storage. Simple and familiar. ChunkedEventDataFrame \u2014 Polars-backed storage with configurable row retention and automatic compaction. Suited for high-volume or streaming workloads. from detectmatelibrary.common.persistency.event_data_structures.dataframes import ( EventDataFrame, ChunkedEventDataFrame, )","title":"DataFrame backends"},{"location":"auxiliar/persistency/#tracker-backends","text":"Track variable behavior over time rather than storing raw data. Useful when a detector needs to understand how variables evolve (e.g., whether they converge to constant values). Is optimized for space efficiency since only extracted features from the logs are stored. EventStabilityTracker \u2014 Classifies each variable as STATIC , STABLE , UNSTABLE , RANDOM , or INSUFFICIENT_DATA based on how its values change over time. from detectmatelibrary.common.persistency.event_data_structures.trackers import ( EventStabilityTracker, )","title":"Tracker backends"},{"location":"auxiliar/persistency/#usage-in-detectors","text":"Persistency is optional . A detector can function without it. When a detector does need to maintain state across events \u2014 for example, to learn normal values during training and flag deviations during detection \u2014 it can integrate persistency by following this pattern:","title":"Usage in detectors"},{"location":"auxiliar/persistency/#1-initialize-in-__init__","text":"Create one or more EventPersistency instances with the appropriate backend. class MyDetector(CoreDetector): def __init__(self, name=\"MyDetector\", config=MyDetectorConfig()): super().__init__(name=name, ...) self.persistency = EventPersistency( event_data_class=EventStabilityTracker, )","title":"1. Initialize in __init__"},{"location":"auxiliar/persistency/#2-accumulate-state-in-train","text":"During training, ingest each event so the backend builds up its internal state. def train(self, input_): variables = self.get_configured_variables(input_, self.config.events) self.persistency.ingest_event( event_id=input_[\"EventID\"], event_template=input_[\"template\"], named_variables=variables, )","title":"2. Accumulate state in train()"},{"location":"auxiliar/persistency/#3-query-state-in-detect","text":"During detection, query the accumulated state to decide whether the incoming event is anomalous. def detect(self, input_, output_): for event_id, backend in self.persistency.get_events_data().items(): stored_data = backend.get_data() # compare input_ against stored_data to produce alerts","title":"3. Query state in detect()"},{"location":"detectors/combo/","text":"Combo Detector The New Combo Value Detector raises alerts when previously unseen combinations of values appear in configured fields (for example new user names, IP addresses, or process names). It is useful to detect novelty, configuration drift, or the appearance of new actors in the environment. Schema Description Input ParserSchema Structured log Output DetectorSchema Combined alert / finding Description This detector maintains a lightweight set of observed combination of values per monitored fields and emits an alert when a combination is not present in the set seen for the first time (subject to configuration). Configuration detectors: NewValueComboDetector: method_type: new_value_combo_detector auto_config: False params: comb_size: 3 events: 1: test: params: {} variables: - pos: 0 name: var1 header_variables: - pos: level Example usage from detectmatelibrary.detectors.combo_detector import ComboDetector, ComboConfig import detectmatelibrary.schemas as schemas test_data = schemas.ParserSchema({ \"parserType\": \"test\", \"EventID\": 12, \"template\": \"test template\", \"variables\": [\"adsasd\", \"asdasd\"], \"logID\": \"2\", \"parsedLogID\": \"2\", \"parserID\": \"test_parser\", \"log\": \"test log message\", \"logFormatVariables\": {\"level\": \"CRITICAL\"} }) output = schemas.DetectorSchema() result = detector.detect(test_data, output) Go back Index","title":"Combo Detector"},{"location":"detectors/combo/#combo-detector","text":"The New Combo Value Detector raises alerts when previously unseen combinations of values appear in configured fields (for example new user names, IP addresses, or process names). It is useful to detect novelty, configuration drift, or the appearance of new actors in the environment. Schema Description Input ParserSchema Structured log Output DetectorSchema Combined alert / finding","title":"Combo Detector"},{"location":"detectors/combo/#description","text":"This detector maintains a lightweight set of observed combination of values per monitored fields and emits an alert when a combination is not present in the set seen for the first time (subject to configuration).","title":"Description"},{"location":"detectors/combo/#configuration","text":"detectors: NewValueComboDetector: method_type: new_value_combo_detector auto_config: False params: comb_size: 3 events: 1: test: params: {} variables: - pos: 0 name: var1 header_variables: - pos: level","title":"Configuration"},{"location":"detectors/combo/#example-usage","text":"from detectmatelibrary.detectors.combo_detector import ComboDetector, ComboConfig import detectmatelibrary.schemas as schemas test_data = schemas.ParserSchema({ \"parserType\": \"test\", \"EventID\": 12, \"template\": \"test template\", \"variables\": [\"adsasd\", \"asdasd\"], \"logID\": \"2\", \"parsedLogID\": \"2\", \"parserID\": \"test_parser\", \"log\": \"test log message\", \"logFormatVariables\": {\"level\": \"CRITICAL\"} }) output = schemas.DetectorSchema() result = detector.detect(test_data, output) Go back Index","title":"Example usage"},{"location":"detectors/new_value/","text":"New Value Detector The New Value Detector raises alerts when previously unseen values appear in configured fields (for example new user names, IP addresses, or process names). It is useful to detect novelty, configuration drift, or the appearance of new actors in the environment. Schema Description Input ParserSchema Structured log Output DetectorSchema Alert / finding Description This detector maintains a lightweight set of observed values per monitored field and emits an alert when a value not present in the set is seen for the first time (subject to configuration). . Configuration example detectors: NewValueDetector: method_type: new_value_detector auto_config: False params: {} events: 1: test: params: {} variables: - pos: 0 name: var1 params: threshold: 0. header_variables: - pos: level params: {} Example usage from detectmatelibrary.detectors.new_value_detector import NewValueDetector, BufferMode import detectmatelibrary.schemas as schemas detector = NewValueDetector(name=\"NewValueTest\", config=cfg) parser_data = schemas.ParserSchema({ \"parserType\": \"test\", \"EventID\": 1, \"template\": \"test template\", \"variables\": [\"var1\"], \"logID\": \"1\", \"parsedLogID\": \"1\", \"parserID\": \"test_parser\", \"log\": \"test log message\", \"logFormatVariables\": {\"timestamp\": \"123456\"} }) alert = detector.process(parsed_data) Go back Index","title":"New Value"},{"location":"detectors/new_value/#new-value-detector","text":"The New Value Detector raises alerts when previously unseen values appear in configured fields (for example new user names, IP addresses, or process names). It is useful to detect novelty, configuration drift, or the appearance of new actors in the environment. Schema Description Input ParserSchema Structured log Output DetectorSchema Alert / finding","title":"New Value Detector"},{"location":"detectors/new_value/#description","text":"This detector maintains a lightweight set of observed values per monitored field and emits an alert when a value not present in the set is seen for the first time (subject to configuration). .","title":"Description"},{"location":"detectors/new_value/#configuration-example","text":"detectors: NewValueDetector: method_type: new_value_detector auto_config: False params: {} events: 1: test: params: {} variables: - pos: 0 name: var1 params: threshold: 0. header_variables: - pos: level params: {}","title":"Configuration example"},{"location":"detectors/new_value/#example-usage","text":"from detectmatelibrary.detectors.new_value_detector import NewValueDetector, BufferMode import detectmatelibrary.schemas as schemas detector = NewValueDetector(name=\"NewValueTest\", config=cfg) parser_data = schemas.ParserSchema({ \"parserType\": \"test\", \"EventID\": 1, \"template\": \"test template\", \"variables\": [\"var1\"], \"logID\": \"1\", \"parsedLogID\": \"1\", \"parserID\": \"test_parser\", \"log\": \"test log message\", \"logFormatVariables\": {\"timestamp\": \"123456\"} }) alert = detector.process(parsed_data) Go back Index","title":"Example usage"},{"location":"detectors/random_detector/","text":"Random Detector The Random Detector produces randomized alerts for incoming parsed logs. It is useful for testing pipelines, alert routing, and downstream consumers without needing a real detection model. Schema Description Input ParserSchema Structured log Output DetectorSchema Generated alerts Description The detector inspects incoming ParserSchema instances and, according to its configuration, emits alerts with synthetic content. It can be configured to sample specific log variables, set thresholds or control alert frequency. Use it for integration testing, load testing, or as a simple example of a detector implementation. Configuration example RandomDetector: method_type: random_detector auto_config: False params: {} events: 1: test: params: {} variables: - pos: 0 name: var1 params: threshold: 0. header_variables: - pos: level params: {} Example usage from detectmatelibrary.detectors.random_detector import RandomDetector import detectmatelibrary.schemas as schemas # assume `config` is loaded from YAML and converted to the detector Config class detector = RandomDetector(name=\"TestDetector\", config=config) parser_data = schemas.ParserSchema({ \"parserType\": \"test\", \"EventID\": 1, \"template\": \"test template\", \"variables\": [\"var1\"], \"logID\": \"1\", \"parsedLogID\": \"1\", \"parserID\": \"test_parser\", \"log\": \"test log message\", \"logFormatVariables\": {\"timestamp\": \"123456\"} }) # process returns True if an alert was emitted, False otherwise alert_emitted = detector.process(parser_data) Go back Index","title":"Random Detector"},{"location":"detectors/random_detector/#random-detector","text":"The Random Detector produces randomized alerts for incoming parsed logs. It is useful for testing pipelines, alert routing, and downstream consumers without needing a real detection model. Schema Description Input ParserSchema Structured log Output DetectorSchema Generated alerts","title":"Random Detector"},{"location":"detectors/random_detector/#description","text":"The detector inspects incoming ParserSchema instances and, according to its configuration, emits alerts with synthetic content. It can be configured to sample specific log variables, set thresholds or control alert frequency. Use it for integration testing, load testing, or as a simple example of a detector implementation.","title":"Description"},{"location":"detectors/random_detector/#configuration-example","text":"RandomDetector: method_type: random_detector auto_config: False params: {} events: 1: test: params: {} variables: - pos: 0 name: var1 params: threshold: 0. header_variables: - pos: level params: {}","title":"Configuration example"},{"location":"detectors/random_detector/#example-usage","text":"from detectmatelibrary.detectors.random_detector import RandomDetector import detectmatelibrary.schemas as schemas # assume `config` is loaded from YAML and converted to the detector Config class detector = RandomDetector(name=\"TestDetector\", config=config) parser_data = schemas.ParserSchema({ \"parserType\": \"test\", \"EventID\": 1, \"template\": \"test template\", \"variables\": [\"var1\"], \"logID\": \"1\", \"parsedLogID\": \"1\", \"parserID\": \"test_parser\", \"log\": \"test log message\", \"logFormatVariables\": {\"timestamp\": \"123456\"} }) # process returns True if an alert was emitted, False otherwise alert_emitted = detector.process(parser_data) Go back Index","title":"Example usage"},{"location":"helper/from_to/","text":"From / To helper Utility methods to help developers load and save different schema objects. Supported formats: Log files : Read-only. Load plain log files and convert entries to LogSchema objects. Binary files : Files that store the serialized bytes of schema objects. JSON files : Files that store schema objects in JSON format. YAML files : Files that store schema objects in YAML format. From The From class is responsible for loading different input formats. class From: @staticmethod def log( component: CoreComponent, in_path: str, do_process: bool = True ) -> Iterator[BaseSchema]: \"\"\"Load logs as input schemas.\"\"\" @staticmethod def binary_file( component: CoreComponent, in_path: str, do_process: bool = True ) -> Iterator[BaseSchema]: \"\"\"Load binary files as input schemas.\"\"\" @staticmethod def json( component: CoreComponent, in_path: str, do_process: bool = True ) -> Iterator[BaseSchema]: \"\"\"Load JSON files as input schemas.\"\"\" @staticmethod def yaml( component: CoreComponent, in_path: str, do_process: bool = True ) -> Iterator[BaseSchema]: \"\"\"Load YAML files as input schemas.\"\"\" Usage parser = DummyParser() for log in From.log(parser, in_path=log_path, do_process=False): print(log) To The To class is responsible for saving schema objects to files. class To: @staticmethod def binary_file(out_: BaseSchema | bytes | None, out_path: str) -> bytes | None: \"\"\"Save output schema to a binary file.\"\"\" @staticmethod def json(out_: BaseSchema | None, out_path: str) -> BaseSchema | None: \"\"\"Save output schema to a JSON file.\"\"\" @staticmethod def yaml(out_: BaseSchema | None, out_path: str) -> BaseSchema | None: \"\"\"Save output schema to a YAML file.\"\"\" Usage parser = DummyParser() for log in From.log(parser, in_path=log_path, do_process=False): assert To.json(log, output_path) == output_schema Example JSON save file format: { \"0\": { \"logID\": \"0\", \"hostname\": \"\", \"log\": \"pid=<*> uid=<*> auid=<*> ses=<*> msg='op=<*> acct=<*> exe=<*> hostname=<*> addr=<*> terminal=<*> res=<*>'\", \"logSource\": \"\", \"__version__\": \"1.0.0\" }, \"1\": { \"logID\": \"1\", \"hostname\": \"\", \"log\": \"pid=<*> uid=<*> auid=<*> ses=<*> msg='unit=<*> comm=<*> exe=<*> hostname=<*> addr=<*> terminal=<*> res=<*>'\", \"logSource\": \"\", \"__version__\": \"1.0.0\" } } FromTo The FromTo class loads and saves inputs and outputs in a single operation. class FromTo: @staticmethod def log2binary_file(component: CoreComponent, in_path: str, out_path: str) -> Iterator[BaseSchema]: \"\"\"Load a log file and save it to a binary file.\"\"\" @staticmethod def log2json(component: CoreComponent, in_path: str, out_path: str) -> Iterator[BaseSchema]: \"\"\"Load a log file and save it to a JSON file.\"\"\" @staticmethod def log2yaml(component: CoreComponent, in_path: str, out_path: str) -> Iterator[BaseSchema]: \"\"\"Load a log file and save it to a YAML file.\"\"\" @staticmethod def binary_file2binary_file( component: CoreComponent, in_path: str, out_path: str ) -> Iterator[BaseSchema]: \"\"\"Load a binary file and save it to a binary file.\"\"\" @staticmethod def binary_file2json( component: CoreComponent, in_path: str, out_path: str ) -> Iterator[BaseSchema]: \"\"\"Load a binary file and save it to a JSON file.\"\"\" @staticmethod def binary_file2yaml( component: CoreComponent, in_path: str, out_path: str ) -> Iterator[BaseSchema]: \"\"\"Load a binary file and save it to a YAML file.\"\"\" @staticmethod def json2binary_file( component: CoreComponent, in_path: str, out_path: str ) -> Iterator[BaseSchema]: \"\"\"Load a JSON file and save it to a binary file.\"\"\" @staticmethod def json2json( component: CoreComponent, in_path: str, out_path: str ) -> Iterator[BaseSchema]: \"\"\"Load a JSON file and save it to a JSON file.\"\"\" @staticmethod def json2yaml( component: CoreComponent, in_path: str, out_path: str ) -> Iterator[BaseSchema]: \"\"\"Load a JSON file and save it to a YAML file.\"\"\" @staticmethod def yaml2binary_file( component: CoreComponent, in_path: str, out_path: str ) -> Iterator[BaseSchema]: \"\"\"Load a YAML file and save it to a binary file.\"\"\" @staticmethod def yaml2json( component: CoreComponent, in_path: str, out_path: str ) -> Iterator[BaseSchema]: \"\"\"Load a YAML file and save it to a JSON file.\"\"\" @staticmethod def yaml2yaml( component: CoreComponent, in_path: str, out_path: str ) -> Iterator[BaseSchema]: \"\"\"Load a YAML file and save it to a YAML file.\"\"\" Usage parser = DummyParser() for parsed_log in FromTo.json2json(parser, log_path, json_path): pass Example input data: { \"0\": { \"logID\": \"0\", \"hostname\": \"\", \"log\": \"pid=<*> uid=<*> auid=<*> ses=<*> msg='op=<*> acct=<*> exe=<*> hostname=<*> addr=<*> terminal=<*> res=<*>'\", \"logSource\": \"\", \"__version__\": \"1.0.0\" }, \"1\": { \"logID\": \"1\", \"hostname\": \"\", \"log\": \"pid=<*> uid=<*> auid=<*> ses=<*> msg='unit=<*> comm=<*> exe=<*> hostname=<*> addr=<*> terminal=<*> res=<*>'\", \"logSource\": \"\", \"__version__\": \"1.0.0\" } } Example output data after parsing: { \"0\": { \"template\": \"This is a dummy template\", \"parsedTimestamp\": 1771336089, \"EventID\": 2, \"logFormatVariables\": { \"Time\": \"0\" }, \"parserID\": \"DummyParser\", \"parserType\": \"dummy_parser\", \"log\": \"pid=<*> uid=<*> auid=<*> ses=<*> msg='op=<*> acct=<*> exe=<*> hostname=<*> addr=<*> terminal=<*> res=<*>'\", \"variables\": [ \"dummy_variable\" ], \"receivedTimestamp\": 1771336089, \"logID\": \"0\", \"__version__\": \"1.0.0\", \"parsedLogID\": \"10\" }, \"1\": { \"template\": \"This is a dummy template\", \"parsedTimestamp\": 1771336089, \"EventID\": 2, \"logFormatVariables\": { \"Time\": \"0\" }, \"parserID\": \"DummyParser\", \"parserType\": \"dummy_parser\", \"log\": \"pid=<*> uid=<*> auid=<*> ses=<*> msg='unit=<*> comm=<*> exe=<*> hostname=<*> addr=<*> terminal=<*> res=<*>'\", \"variables\": [ \"dummy_variable\" ], \"receivedTimestamp\": 1771336089, \"logID\": \"1\", \"__version__\": \"1.0.0\", \"parsedLogID\": \"11\" } } Go back to Index","title":"From to"},{"location":"helper/from_to/#from-to-helper","text":"Utility methods to help developers load and save different schema objects. Supported formats: Log files : Read-only. Load plain log files and convert entries to LogSchema objects. Binary files : Files that store the serialized bytes of schema objects. JSON files : Files that store schema objects in JSON format. YAML files : Files that store schema objects in YAML format.","title":"From / To helper"},{"location":"helper/from_to/#from","text":"The From class is responsible for loading different input formats. class From: @staticmethod def log( component: CoreComponent, in_path: str, do_process: bool = True ) -> Iterator[BaseSchema]: \"\"\"Load logs as input schemas.\"\"\" @staticmethod def binary_file( component: CoreComponent, in_path: str, do_process: bool = True ) -> Iterator[BaseSchema]: \"\"\"Load binary files as input schemas.\"\"\" @staticmethod def json( component: CoreComponent, in_path: str, do_process: bool = True ) -> Iterator[BaseSchema]: \"\"\"Load JSON files as input schemas.\"\"\" @staticmethod def yaml( component: CoreComponent, in_path: str, do_process: bool = True ) -> Iterator[BaseSchema]: \"\"\"Load YAML files as input schemas.\"\"\"","title":"From"},{"location":"helper/from_to/#usage","text":"parser = DummyParser() for log in From.log(parser, in_path=log_path, do_process=False): print(log)","title":"Usage"},{"location":"helper/from_to/#to","text":"The To class is responsible for saving schema objects to files. class To: @staticmethod def binary_file(out_: BaseSchema | bytes | None, out_path: str) -> bytes | None: \"\"\"Save output schema to a binary file.\"\"\" @staticmethod def json(out_: BaseSchema | None, out_path: str) -> BaseSchema | None: \"\"\"Save output schema to a JSON file.\"\"\" @staticmethod def yaml(out_: BaseSchema | None, out_path: str) -> BaseSchema | None: \"\"\"Save output schema to a YAML file.\"\"\"","title":"To"},{"location":"helper/from_to/#usage_1","text":"parser = DummyParser() for log in From.log(parser, in_path=log_path, do_process=False): assert To.json(log, output_path) == output_schema Example JSON save file format: { \"0\": { \"logID\": \"0\", \"hostname\": \"\", \"log\": \"pid=<*> uid=<*> auid=<*> ses=<*> msg='op=<*> acct=<*> exe=<*> hostname=<*> addr=<*> terminal=<*> res=<*>'\", \"logSource\": \"\", \"__version__\": \"1.0.0\" }, \"1\": { \"logID\": \"1\", \"hostname\": \"\", \"log\": \"pid=<*> uid=<*> auid=<*> ses=<*> msg='unit=<*> comm=<*> exe=<*> hostname=<*> addr=<*> terminal=<*> res=<*>'\", \"logSource\": \"\", \"__version__\": \"1.0.0\" } }","title":"Usage"},{"location":"helper/from_to/#fromto","text":"The FromTo class loads and saves inputs and outputs in a single operation. class FromTo: @staticmethod def log2binary_file(component: CoreComponent, in_path: str, out_path: str) -> Iterator[BaseSchema]: \"\"\"Load a log file and save it to a binary file.\"\"\" @staticmethod def log2json(component: CoreComponent, in_path: str, out_path: str) -> Iterator[BaseSchema]: \"\"\"Load a log file and save it to a JSON file.\"\"\" @staticmethod def log2yaml(component: CoreComponent, in_path: str, out_path: str) -> Iterator[BaseSchema]: \"\"\"Load a log file and save it to a YAML file.\"\"\" @staticmethod def binary_file2binary_file( component: CoreComponent, in_path: str, out_path: str ) -> Iterator[BaseSchema]: \"\"\"Load a binary file and save it to a binary file.\"\"\" @staticmethod def binary_file2json( component: CoreComponent, in_path: str, out_path: str ) -> Iterator[BaseSchema]: \"\"\"Load a binary file and save it to a JSON file.\"\"\" @staticmethod def binary_file2yaml( component: CoreComponent, in_path: str, out_path: str ) -> Iterator[BaseSchema]: \"\"\"Load a binary file and save it to a YAML file.\"\"\" @staticmethod def json2binary_file( component: CoreComponent, in_path: str, out_path: str ) -> Iterator[BaseSchema]: \"\"\"Load a JSON file and save it to a binary file.\"\"\" @staticmethod def json2json( component: CoreComponent, in_path: str, out_path: str ) -> Iterator[BaseSchema]: \"\"\"Load a JSON file and save it to a JSON file.\"\"\" @staticmethod def json2yaml( component: CoreComponent, in_path: str, out_path: str ) -> Iterator[BaseSchema]: \"\"\"Load a JSON file and save it to a YAML file.\"\"\" @staticmethod def yaml2binary_file( component: CoreComponent, in_path: str, out_path: str ) -> Iterator[BaseSchema]: \"\"\"Load a YAML file and save it to a binary file.\"\"\" @staticmethod def yaml2json( component: CoreComponent, in_path: str, out_path: str ) -> Iterator[BaseSchema]: \"\"\"Load a YAML file and save it to a JSON file.\"\"\" @staticmethod def yaml2yaml( component: CoreComponent, in_path: str, out_path: str ) -> Iterator[BaseSchema]: \"\"\"Load a YAML file and save it to a YAML file.\"\"\"","title":"FromTo"},{"location":"helper/from_to/#usage_2","text":"parser = DummyParser() for parsed_log in FromTo.json2json(parser, log_path, json_path): pass Example input data: { \"0\": { \"logID\": \"0\", \"hostname\": \"\", \"log\": \"pid=<*> uid=<*> auid=<*> ses=<*> msg='op=<*> acct=<*> exe=<*> hostname=<*> addr=<*> terminal=<*> res=<*>'\", \"logSource\": \"\", \"__version__\": \"1.0.0\" }, \"1\": { \"logID\": \"1\", \"hostname\": \"\", \"log\": \"pid=<*> uid=<*> auid=<*> ses=<*> msg='unit=<*> comm=<*> exe=<*> hostname=<*> addr=<*> terminal=<*> res=<*>'\", \"logSource\": \"\", \"__version__\": \"1.0.0\" } } Example output data after parsing: { \"0\": { \"template\": \"This is a dummy template\", \"parsedTimestamp\": 1771336089, \"EventID\": 2, \"logFormatVariables\": { \"Time\": \"0\" }, \"parserID\": \"DummyParser\", \"parserType\": \"dummy_parser\", \"log\": \"pid=<*> uid=<*> auid=<*> ses=<*> msg='op=<*> acct=<*> exe=<*> hostname=<*> addr=<*> terminal=<*> res=<*>'\", \"variables\": [ \"dummy_variable\" ], \"receivedTimestamp\": 1771336089, \"logID\": \"0\", \"__version__\": \"1.0.0\", \"parsedLogID\": \"10\" }, \"1\": { \"template\": \"This is a dummy template\", \"parsedTimestamp\": 1771336089, \"EventID\": 2, \"logFormatVariables\": { \"Time\": \"0\" }, \"parserID\": \"DummyParser\", \"parserType\": \"dummy_parser\", \"log\": \"pid=<*> uid=<*> auid=<*> ses=<*> msg='unit=<*> comm=<*> exe=<*> hostname=<*> addr=<*> terminal=<*> res=<*>'\", \"variables\": [ \"dummy_variable\" ], \"receivedTimestamp\": 1771336089, \"logID\": \"1\", \"__version__\": \"1.0.0\", \"parsedLogID\": \"11\" } } Go back to Index","title":"Usage"},{"location":"parsers/json_parser/","text":"JSON Parser Extracts structured information from JSON-formatted logs. Optionally delegates parsing of a specific JSON field (the \"content\") to another parser (for example, the Template matcher). Schema Description Input LogSchema Raw log (JSON string) Output ParserSchema Structured log with extracted fields Configuration Relevant config options (example names used by the implementation): method_type (string): parser type identifier (e.g. json_parser ). params.timestamp_name (string | null): JSON key to use as the received/parsed timestamp. params.content_name (string): JSON key that contains the textual content to parse further (default \"message\" or \"content\" ). params.flatten_nested (bool, default True): flatten nested objects into dot-separated keys in logFormatVariables . params.content_parser (string | dict): optional parser spec (name or config) to parse the extracted content. params.ignore_parse_errors (bool, default True): if True, parser returns gracefully on JSON errors instead of raising. Example YAML fragment: parsers: JsonParser: method_type: json_parser params: timestamp_name: \"time\" content_name: \"message\" flatten_nested: True content_parser: method_type: matcher_parser params: path_templates: tests/test_templates.txt ignore_parse_errors: True Usage examples Basic usage \u2014 parse JSON and extract fields: from detectmatelibrary.parsers.json_parser import JsonParser import detectmatelibrary.schemas as schemas config = JsonParserConfig() parser = JsonParser(name=\"TestParser\", config=config) json_log = { \"time\": \"2023-11-18 10:30:00\", \"request\": { \"method\": \"GET\", \"path\": \"/api/users\", \"headers\": { \"content-type\": \"application/json\" } } } input_log = schemas.LogSchema({ \"logID\": \"1\", \"log\": json.dumps(json_log) }) output = schemas.ParserSchema() parser.parse(input_log, output) print(output.logFormatVariables[\"request.method\"]) # \"GET\" print(output.logFormatVariables[\"request.path\"]) #\"/api/users\" Go back Index","title":"Json Parser"},{"location":"parsers/json_parser/#json-parser","text":"Extracts structured information from JSON-formatted logs. Optionally delegates parsing of a specific JSON field (the \"content\") to another parser (for example, the Template matcher). Schema Description Input LogSchema Raw log (JSON string) Output ParserSchema Structured log with extracted fields","title":"JSON Parser"},{"location":"parsers/json_parser/#configuration","text":"Relevant config options (example names used by the implementation): method_type (string): parser type identifier (e.g. json_parser ). params.timestamp_name (string | null): JSON key to use as the received/parsed timestamp. params.content_name (string): JSON key that contains the textual content to parse further (default \"message\" or \"content\" ). params.flatten_nested (bool, default True): flatten nested objects into dot-separated keys in logFormatVariables . params.content_parser (string | dict): optional parser spec (name or config) to parse the extracted content. params.ignore_parse_errors (bool, default True): if True, parser returns gracefully on JSON errors instead of raising. Example YAML fragment: parsers: JsonParser: method_type: json_parser params: timestamp_name: \"time\" content_name: \"message\" flatten_nested: True content_parser: method_type: matcher_parser params: path_templates: tests/test_templates.txt ignore_parse_errors: True","title":"Configuration"},{"location":"parsers/json_parser/#usage-examples","text":"Basic usage \u2014 parse JSON and extract fields: from detectmatelibrary.parsers.json_parser import JsonParser import detectmatelibrary.schemas as schemas config = JsonParserConfig() parser = JsonParser(name=\"TestParser\", config=config) json_log = { \"time\": \"2023-11-18 10:30:00\", \"request\": { \"method\": \"GET\", \"path\": \"/api/users\", \"headers\": { \"content-type\": \"application/json\" } } } input_log = schemas.LogSchema({ \"logID\": \"1\", \"log\": json.dumps(json_log) }) output = schemas.ParserSchema() parser.parse(input_log, output) print(output.logFormatVariables[\"request.method\"]) # \"GET\" print(output.logFormatVariables[\"request.path\"]) #\"/api/users\" Go back Index","title":"Usage examples"},{"location":"parsers/template_matcher/","text":"Template matcher Parser that takes a set of templates and matches them to incoming logs. It extracts parameters from positions marked with the <*> wildcard and returns a ParserSchema with the matched template and the extracted variables. Schema Description Input LogSchema Unstructured log Output ParserSchema Structured log Overview The template matcher is a lightweight, fast parser intended for logs that follow stable textual templates with variable fields. Templates use the token <*> to mark wildcard slots. The matcher: Preprocesses logs and templates (remove spaces, punctuation, lowercase) based on config. Finds the first template that matches and extracts all wildcard parameters in order. Populates ParserSchema fields: EventID , template , variables , logID , and related fields. This parser is deterministic and designed for high-throughput use when templates are known in advance. Template format Templates are plain text lines in a template file. Use <*> for wildcard slots. Example template file (templates.txt): pid=<*> uid=<*> auid=<*> ses=<*> msg='op=PAM:<*> acct=<*> login success: user=<*> source=<*> Configuration Typical MatcherParser config options (fields in config class): method_type : must match the parser type (\"matcher_parser\" or configured name). path_templates : path to the newline-delimited template file. remove_spaces (bool, default True): remove all spaces during matching. remove_punctuation (bool, default True): strip punctuation except the <*> token. lowercase (bool, default True): lowercase logs and templates before matching. auto_config (bool): whether to attempt any auto-configuration phase (not required). Example YAML entry: parsers: MatcherParser: method_type: matcher_parser auto_config: False params: remove_spaces: True remove_punctuation: True lowercase: True path_templates: path/to/templates.txt Usage examples Simple usage \u2014 load templates and match a log: from detectmatelibrary.parsers.template_matcher import MatcherParser from detectmatelibrary import schemas # instantiate parser (config can be a dict or config object) cfg = { \"parsers\": { \"MatcherParser\": { \"method_type\": \"matcher_parser\", \"params\": { \"path_templates\": \"tests/test_folder/test_templates.txt\", \"remove_spaces\": True, \"remove_punctuation\": True, \"lowercase\": True } } } } parser = MatcherParser(name=\"MatcherParser\", config=cfg) # match a log input_log = schemas.LogSchema({\"logID\": \"0\", \"log\": \"pid=9699 uid=0 auid=4294967295 ses=4294967295 msg='op=PAM:accounting acct=\\\"root\\\"'\"}) parsed = parser.process(input_log) # or parser.parse / parser.match depending on wrapper API # parsed is a ParserSchema (or an output container). Check fields: print(parsed.template) # matched template text print(parsed.variables) # list of extracted params Go back to Index","title":"Template Matcher"},{"location":"parsers/template_matcher/#template-matcher","text":"Parser that takes a set of templates and matches them to incoming logs. It extracts parameters from positions marked with the <*> wildcard and returns a ParserSchema with the matched template and the extracted variables. Schema Description Input LogSchema Unstructured log Output ParserSchema Structured log","title":"Template matcher"},{"location":"parsers/template_matcher/#overview","text":"The template matcher is a lightweight, fast parser intended for logs that follow stable textual templates with variable fields. Templates use the token <*> to mark wildcard slots. The matcher: Preprocesses logs and templates (remove spaces, punctuation, lowercase) based on config. Finds the first template that matches and extracts all wildcard parameters in order. Populates ParserSchema fields: EventID , template , variables , logID , and related fields. This parser is deterministic and designed for high-throughput use when templates are known in advance.","title":"Overview"},{"location":"parsers/template_matcher/#template-format","text":"Templates are plain text lines in a template file. Use <*> for wildcard slots. Example template file (templates.txt): pid=<*> uid=<*> auid=<*> ses=<*> msg='op=PAM:<*> acct=<*> login success: user=<*> source=<*>","title":"Template format"},{"location":"parsers/template_matcher/#configuration","text":"Typical MatcherParser config options (fields in config class): method_type : must match the parser type (\"matcher_parser\" or configured name). path_templates : path to the newline-delimited template file. remove_spaces (bool, default True): remove all spaces during matching. remove_punctuation (bool, default True): strip punctuation except the <*> token. lowercase (bool, default True): lowercase logs and templates before matching. auto_config (bool): whether to attempt any auto-configuration phase (not required). Example YAML entry: parsers: MatcherParser: method_type: matcher_parser auto_config: False params: remove_spaces: True remove_punctuation: True lowercase: True path_templates: path/to/templates.txt","title":"Configuration"},{"location":"parsers/template_matcher/#usage-examples","text":"Simple usage \u2014 load templates and match a log: from detectmatelibrary.parsers.template_matcher import MatcherParser from detectmatelibrary import schemas # instantiate parser (config can be a dict or config object) cfg = { \"parsers\": { \"MatcherParser\": { \"method_type\": \"matcher_parser\", \"params\": { \"path_templates\": \"tests/test_folder/test_templates.txt\", \"remove_spaces\": True, \"remove_punctuation\": True, \"lowercase\": True } } } } parser = MatcherParser(name=\"MatcherParser\", config=cfg) # match a log input_log = schemas.LogSchema({\"logID\": \"0\", \"log\": \"pid=9699 uid=0 auid=4294967295 ses=4294967295 msg='op=PAM:accounting acct=\\\"root\\\"'\"}) parsed = parser.process(input_log) # or parser.parse / parser.match depending on wrapper API # parsed is a ParserSchema (or an output container). Check fields: print(parsed.template) # matched template text print(parsed.variables) # list of extracted params Go back to Index","title":"Usage examples"}]}